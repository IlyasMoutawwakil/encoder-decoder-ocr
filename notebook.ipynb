{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -r /usr/users/sdi1/sdi1_29/.cache/huggingface/datasets/wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/sdi1/sdi1_29/micromamba/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting new experiment ocr_fr_128_SwinTransformerEncoder_AutoregressiveTransformerDecoder_AdamW_CosineLRScheduler\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from configs import CHARACTERS, DATASET_NAME, ENCODER_CONFIG, \\\n",
    "    DECODER_CONFIG, OPTIMIZER_CONFIG, SCHEDULER_CONFIG\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "max_train_steps = 1000\n",
    "max_val_steps = 10\n",
    "\n",
    "batch_size = 8\n",
    "num_workers = 8\n",
    "\n",
    "language = 'fr'\n",
    "model_max_length = 128\n",
    "\n",
    "characters = CHARACTERS[language]\n",
    "dataset_name = DATASET_NAME[language]\n",
    "\n",
    "encoder_name = 'SwinTransformerEncoder'\n",
    "encoder_config = ENCODER_CONFIG[encoder_name]\n",
    "\n",
    "decoder_name = 'AutoregressiveTransformerDecoder'\n",
    "decoder_config = DECODER_CONFIG[decoder_name]\n",
    "\n",
    "optimizer_name = 'AdamW'\n",
    "optimizer_config = OPTIMIZER_CONFIG[optimizer_name]\n",
    "\n",
    "scheduler_name = 'CosineLRScheduler'\n",
    "scheduler_config = SCHEDULER_CONFIG[scheduler_name]\n",
    "\n",
    "experiment_name = f\"ocr_{language}_{model_max_length}_{encoder_name}_{decoder_name}_{optimizer_name}_{scheduler_name}\"\n",
    "\n",
    "if os.path.exists(f\"checkpoints/{experiment_name}/\"):\n",
    "    print(f\"Experiment {experiment_name} already exists, resuming training\")\n",
    "    ckpt_path = f\"checkpoints/{experiment_name}/last.ckpt\"\n",
    "else:\n",
    "    print(f\"Starting new experiment {experiment_name}\")\n",
    "    ckpt_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization.tokenizer import CharacterTokenizer\n",
    "tokenizer = CharacterTokenizer(\n",
    "    characters=characters,\n",
    "    model_max_length=model_max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, Grayscale, ToTensor, Normalize\n",
    "transform = Compose([\n",
    "    Resize((encoder_config[\"params\"][\"height\"],\n",
    "            encoder_config[\"params\"][\"width\"])),\n",
    "    Grayscale(),\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing modules for handwritten text generation.\n"
     ]
    }
   ],
   "source": [
    "from dataset.wikipedia_dataset import WikipediaTextLineDataModule\n",
    "datamodule = WikipediaTextLineDataModule(\n",
    "    dataset_name=dataset_name,\n",
    "    transform=transform,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    characters=characters,\n",
    ")\n",
    "datamodule.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/users/sdi1/sdi1_29/micromamba/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from modeling.lightning_wrapper import VisionEncoderLanguageDecoderWrapper\n",
    "lightning_model = VisionEncoderLanguageDecoderWrapper(\n",
    "    tokenizer=tokenizer,\n",
    "    encoder_config=encoder_config,\n",
    "    decoder_config=decoder_config,\n",
    "    optimizer_config=optimizer_config,\n",
    "    scheduler_config=scheduler_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Found cached dataset wikipedia (/usr/users/sdi1/sdi1_29/.cache/huggingface/datasets/wikipedia/20220301.fr/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n",
      "Found cached dataset wikipedia (/usr/users/sdi1/sdi1_29/.cache/huggingface/datasets/wikipedia/20220301.fr/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n",
      "Found cached dataset wikipedia (/usr/users/sdi1/sdi1_29/.cache/huggingface/datasets/wikipedia/20220301.fr/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n",
      "Found cached dataset wikipedia (/usr/users/sdi1/sdi1_29/.cache/huggingface/datasets/wikipedia/20220301.fr/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n",
      "Loading cached processed dataset at /usr/users/sdi1/sdi1_29/.cache/huggingface/datasets/wikipedia/20220301.fr/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559/cache-576bafc1b01b6775_*_of_00008.arrow\n",
      "Loading cached processed dataset at /usr/users/sdi1/sdi1_29/.cache/huggingface/datasets/wikipedia/20220301.fr/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559/cache-909a2f24c1b78827_*_of_00008.arrow\n",
      "\n",
      "  | Name             | Type                             | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | vision_encoder   | SwinTransformerEncoder           | 7.3 M \n",
      "1 | language_decoder | AutoregressiveTransformerDecoder | 9.6 M \n",
      "2 | cer              | CharErrorRate                    | 0     \n",
      "----------------------------------------------------------------------\n",
      "16.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "16.8 M    Total params\n",
      "67.384    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|â–Ž         | 33/1000 [01:33<45:28,  2.82s/it, v_num=2, train_loss=3.540]  "
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "prog_bar = pl.callbacks.progress.TQDMProgressBar(\n",
    "    refresh_rate=1,\n",
    ")\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(\n",
    "    save_dir=f\"logs/{experiment_name}/\",\n",
    ")\n",
    "\n",
    "ckpt_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=f\"checkpoints/{experiment_name}/\",\n",
    "    filename=\"checkpoint-{epoch:03d}-{val_cer:.5f}\",\n",
    "    monitor=\"val_cer\",\n",
    "    save_last=True,\n",
    "    save_top_k=3,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(\n",
    "    logging_interval=\"step\",\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\" if use_cuda else 'cpu',\n",
    "\n",
    "    max_epochs=-1,\n",
    "    log_every_n_steps=1,\n",
    "    num_sanity_val_steps=1,\n",
    "\n",
    "    limit_val_batches=max_val_steps,\n",
    "    limit_train_batches=max_train_steps,\n",
    "\n",
    "    callbacks=[ckpt_callback, lr_monitor, prog_bar],\n",
    "    enable_progress_bar=True,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model=lightning_model,\n",
    "    datamodule=datamodule,\n",
    "    ckpt_path=ckpt_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85c6ade6c056b4aa49aed133be44910d191c2ad4e7376de8c6cbfeddc3110f47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
