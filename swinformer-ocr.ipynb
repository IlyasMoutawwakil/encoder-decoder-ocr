{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [16, 4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training config\n",
    "experiment_name = 'swinformer-ocr'\n",
    "\n",
    "batch_size = 2\n",
    "num_workers = 4\n",
    "\n",
    "max_train_steps = 20000\n",
    "max_val_steps = max_train_steps // 100\n",
    "\n",
    "# environment config\n",
    "import torch\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer config\n",
    "model_max_length = 96\n",
    "bos_token_id = 0\n",
    "eos_token_id = 1\n",
    "pad_token_id = 2\n",
    "unk_token_id = 3\n",
    "\n",
    "# input image config\n",
    "height, width = 128, 640\n",
    "\n",
    "channels = 1\n",
    "pixel_mean = (0.5,) # for one channel\n",
    "pixel_std = (0.5,) # for one channel\n",
    "\n",
    "# encoder architecture config\n",
    "patch_size = 4\n",
    "window_size = 8\n",
    "\n",
    "embed_dim = 96\n",
    "depths = [2, 6, 2]\n",
    "num_heads = [6, 12, 24]\n",
    "\n",
    "# decoder architecture config\n",
    "decoder_config = dict(\n",
    "    dim=384,\n",
    "    depth=4,\n",
    "    heads=8,\n",
    "    cross_attend=True,\n",
    "    ff_glu=False,\n",
    "    attn_on_attn=False,\n",
    "    use_scalenorm=False,\n",
    "    rel_pos_bias=False\n",
    ")\n",
    "\n",
    "# auto regressive wrapper architecture config\n",
    "from dataset.text_cleaning_utils import ALL_CHARACTERS\n",
    "num_tokens = len(ALL_CHARACTERS)\n",
    "max_seq_len = model_max_length\n",
    "\n",
    "# optimizer config\n",
    "from timm.optim import AdamW\n",
    "optimizer_config = dict(\n",
    "    base_class=AdamW,\n",
    "    params=dict(\n",
    "        lr=1e-4,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8,\n",
    "        weight_decay=1e-2,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# schedueler config\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "scheduler_config = dict(\n",
    "    base_class=CosineLRScheduler,\n",
    "    params=dict(\n",
    "        t_initial=200,\n",
    "        lr_min=1e-6,\n",
    "        cycle_mul=3,\n",
    "        cycle_decay=0.8,\n",
    "        cycle_limit=20,\n",
    "        warmup_t=20,\n",
    "        k_decay=1.5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization.character_tokenizer import CharacterTokenizer\n",
    "\n",
    "character_tokenizer = CharacterTokenizer(\n",
    "    characters=ALL_CHARACTERS,\n",
    "    bos_token_id=bos_token_id,\n",
    "    eos_token_id=eos_token_id,\n",
    "    pad_token_id=pad_token_id,\n",
    "    unk_token_id=unk_token_id,\n",
    "    model_max_length=model_max_length,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, Grayscale, ToTensor, Normalize\n",
    "\n",
    "transform = Compose([\n",
    "    Resize((height, width)),\n",
    "    Grayscale(),\n",
    "    ToTensor(),\n",
    "    Normalize(pixel_mean, pixel_std),\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing modules for handwritten text generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikipedia (/home/ilyas/.cache/huggingface/datasets/wikipedia/20220301.fr/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n",
      "Found cached dataset wikipedia (/home/ilyas/.cache/huggingface/datasets/wikipedia/20220301.fr/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n",
      "Loading cached processed dataset at /home/ilyas/.cache/huggingface/datasets/wikipedia/20220301.fr/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559/cache-421afcd459ec1f9d_*_of_00004.arrow\n",
      "Loading cached processed dataset at /home/ilyas/.cache/huggingface/datasets/wikipedia/20220301.fr/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559/cache-c181330655572a4c_*_of_00004.arrow\n"
     ]
    }
   ],
   "source": [
    "import datasets as ds\n",
    "from dataset.textline_dataset import TextLineDataset\n",
    "from dataset.text_cleaning_utils import preprocess_wikipedia_dataset\n",
    "\n",
    "train_dataset = ds.load_dataset(\"wikipedia\", \"20220301.fr\", split=\"train[:90%]\")\n",
    "val_dataset = ds.load_dataset(\"wikipedia\", \"20220301.fr\", split=\"train[-10%:]\")\n",
    "\n",
    "train_dataset = preprocess_wikipedia_dataset(train_dataset)\n",
    "val_dataset = preprocess_wikipedia_dataset(val_dataset)\n",
    "\n",
    "train_dataset = TextLineDataset(\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=character_tokenizer,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "val_dataset = TextLineDataset(\n",
    "    val_dataset,\n",
    "    tokenizer=character_tokenizer,\n",
    "    transform=transform,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilyas/micromamba/envs/transformers-ocr/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3488.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from modeling.lightning_base import LightningBase\n",
    "from modeling.encoder import SwinTransformerEncoder\n",
    "from modeling.decoder import AutoregressiveDecoder\n",
    "from modeling.vision_encoder_decoder import VisionEncoderDecoder\n",
    "\n",
    "# create encoder\n",
    "encoder = SwinTransformerEncoder(\n",
    "    img_size=(height, width),\n",
    "    patch_size=patch_size,\n",
    "    in_chans=channels,\n",
    "    embed_dim=embed_dim,\n",
    "    depths=depths,\n",
    "    num_heads=num_heads,\n",
    "    window_size=window_size,\n",
    ")\n",
    "\n",
    "# create decoder\n",
    "decoder = AutoregressiveDecoder(\n",
    "    decoder_config=decoder_config,\n",
    "\n",
    "    num_tokens=character_tokenizer.vocab_size,\n",
    "    max_seq_len=character_tokenizer.model_max_length,\n",
    "\n",
    "    bos_token_id=character_tokenizer.bos_token_id,\n",
    "    eos_token_id=character_tokenizer.eos_token_id,\n",
    "    pad_token_id=character_tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "# create vision encoder decoder\n",
    "vision_encoder_decoder = VisionEncoderDecoder(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    ")\n",
    "\n",
    "# create lightning model\n",
    "lightning_model = LightningBase(\n",
    "    tokenizer=character_tokenizer,\n",
    "    model=vision_encoder_decoder,\n",
    "    optimizer_config=optimizer_config,\n",
    "    scheduler_config=scheduler_config,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=use_cuda, # for faster cpu to gpu transfer\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=use_cuda, # for faster cpu to gpu transfer\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "prog_bar = pl.callbacks.progress.TQDMProgressBar(\n",
    "    refresh_rate=10,\n",
    ")\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(\n",
    "    save_dir=f\"logs/{experiment_name}/\",\n",
    ")\n",
    "\n",
    "ckpt_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=f\"checkpoints/{experiment_name}/\",\n",
    "    filename=\"checkpoint-{epoch:03d}-{val_cer:.5f}\",\n",
    "    monitor=\"val_cer\",\n",
    "    save_last=True,\n",
    "    save_top_k=3,\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(\n",
    "    logging_interval=\"step\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\" if use_cuda else None,\n",
    "    benchmark=True,\n",
    "\n",
    "    log_every_n_steps=1,\n",
    "    # num_sanity_val_steps=1,\n",
    "\n",
    "    limit_val_batches=max_val_steps,\n",
    "    limit_train_batches=max_train_steps,\n",
    "\n",
    "    callbacks=[ckpt_callback, lr_monitor, prog_bar],\n",
    "    enable_progress_bar=True,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    lightning_model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85c6ade6c056b4aa49aed133be44910d191c2ad4e7376de8c6cbfeddc3110f47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
